{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting:  Data Wrangling\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet archive of the Twitter user @dog rates,better known as WeRateDogs, is the dataset examined in this project.\n",
    "WeRateDogs is a twitter_account that rates people's dogs with humorous comment about the dog.\n",
    "\n",
    "Contrary to what I thought before I started, manipulating the WeRateDogs dataset turned out to be a rather difficult and time-consuming operation. Not only did I get to put what I had learned into practice during this time, but I also honed my googling abilities by understanding how to use the different function features and how to combine them to complete what could otherwise seem to be very difficult tasks.\n",
    "\n",
    "\n",
    "Goals for the project included:\n",
    "\n",
    " -  Wrangling the twitter data through the following steps: \n",
    "      - Gathering Data \n",
    "      - Assessing Data \n",
    "      - Cleaning Data \n",
    "    \n",
    "    \n",
    "    \n",
    " -   Stroing, analysing and visualising the cleaned data \n",
    "    \n",
    "    \n",
    "  -  Reporting on the various steps taken under the wrangling section \n",
    "\n",
    "\n",
    "### Gathering Data\n",
    "\n",
    "The Data Wrangling started with getting data from 3 different sources.\n",
    "\n",
    "\n",
    " -   The project environment already had the unclean version of the Twitter archive enhanced dataset available.  This archive contains basic tweet data (tweet_id, timestamp, text etc) for all 5000+ of their tweets as recorded on August 1, 2017.\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    " -  The image_prediction table was to be downloaded from the internet programmatically using the url that was provided. I utilized the request library for this task. With the help of the specified url, the library was used to retrieve all the content, which was then written to the image predictions.tsv file using the get method.\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    " -  The WeRateDogs tweet data was gathered using the Twitter API and Python's tweepy library. The API could only be queried for the necessary information with authorization and elevated access. Unfortunately, the Twitter developer team refused to provide me with elevated access, so I chose the alternative stated in the project motivation section and manually downloaded the tweet json.txt file.  Then, after converting the file's content into JSON format, I read each tweet's JSON object line by line and saved it to an empty Python list. Accessing all the json objects was done with the intention of obtaining the favorite and retweet counts for each tweet. A pandas dataframe was created using the Python list containing the tweet data and their corresponding tweet ids.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Assessing Data\n",
    "\n",
    "After the data was gathered from the various sources, the data was Visually and Programmatically assessed on both Quality and Tidiness issues.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " #### Quality Issue:\n",
    "*****\n",
    "\n",
    "\n",
    "\n",
    "**twitter_archive_enhance.csv** :\n",
    "\n",
    "-  Completeness:\n",
    "  \n",
    "      - Missing information in the following columns: in_reply_to_user_id, in_reply_to_status_id, retweeted_status_id,   retweeted_status_user_id, retweeted_status_timestamp, expandanded_urls.\n",
    " \n",
    "      - The tweet_id columns being an integer instead of string.  This applies to all tables.   \n",
    "      \n",
    "\n",
    "\n",
    "-  Validity:\n",
    "   \n",
    "      - Some of the dog have 'None', 'a' or 'an' as names.   \n",
    "  \n",
    "      - The dataset contains retweets, which means there is duplicated data. \n",
    "      \n",
    "      \n",
    " \n",
    "-  Accuracy:\n",
    "   \n",
    "      - Timestamp column is of a string type instead of datetime.   \n",
    "      \n",
    "      \n",
    "  \n",
    "-  Consistency:\n",
    " \n",
    "     - The Source column still has HTML tags.   \n",
    "\n",
    "\n",
    "\n",
    "**image_predictions.tsv** :\n",
    "\n",
    "-  Consistency:\n",
    "  \n",
    "      - Underscores instead of spaces in p1, p2, p3 columns.\n",
    "\n",
    "      - Some breed names starts with uppercase, others with lowercase in p1, p2, p3 columns.\n",
    "\n",
    "      - The values in p1_conf, p2_conf, p3_conf columns are in proportions instead of percentages\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "**tweet_json.txt** :\n",
    "\n",
    "-  Completeness:\n",
    "  \n",
    "      - Missing some data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " #### Tidiness Issue:\n",
    "****\n",
    "\n",
    "**twitter_archive_enhance.csv** :\n",
    "+ `doggo, floofer, pupper` and `puppo` columns are in a seperate columns instead of a single column. This violate the \"each variable forms a column\" requirement.\n",
    "\n",
    "\n",
    "**image_predictions.tsv** :\n",
    "+ Image predictions table should be added to twitter archive table.\n",
    "\n",
    "**tweet_json.txt** :\n",
    "+ The columns (`retweet_count, favorite_count, followers_count`) should be added to twitter archive table.\n",
    "\n",
    "\n",
    "\n",
    "### Cleaning Data\n",
    "\n",
    "The cleaning part was the most code oriented. For each of the issue identified, I used the Define, Code and Test format wherein I first defined the operation I wanted to perform, then wrote the code to carry out that operation and finally wrote test codes to verify if the code works as expected.\n",
    "\n",
    "Various pandas functions were explored and used, some in isolation and others in combination with other functions. A lot of the operations involved extracting data from certain features and replacing inaccurate or incomplete column data. Also some of the rows and columns were dropped and others combined to form a single column with the ultimate purpose of coming out with a quality and tidy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
